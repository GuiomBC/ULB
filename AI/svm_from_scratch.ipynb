{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Support Vector Machine</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code of the cost function:\n",
    "$$J(w) = \\frac{1}{2}||w||^2 + C\\left[\\frac{1}{N}\\sum\\limits_{i}^{n}max(0, 1-y_i * (w \\cdotp x_i + b))\\right]$$\n",
    "The first part of the function corresponds to the margin, in fact the width between the two (positive and negative) hyperplanes is equal to: \n",
    "$$width = (x_+ - x_-)* \\frac{w}{||w||}$$ and joining the previous equation with the following equations of the two hyperplanes:\n",
    "$$y_i*(w x_+ + b) -1 = 0$$ and $$ y_i*(w x_- + b) -1 = 0$$ with $$y_i = \\begin{cases}\n",
    "  1 & \\text{for } x_+  \\\\   \n",
    "  -1 & \\text{for } x_-\n",
    "\\end{cases}$$\n",
    "we obtain $$width = \\frac{2}{||w||}$$ and we have to maximize the width which is the same to minimize w and the trick is to transform  $$\\text{min }w$$ into $$ min\\frac{1}{2}||w||^2$$ \n",
    "The second part of the function which begins by C is called Hinge loss function and we have to minimize the sum which corresponds to distance between positive (or negative) hyperplane and our training set. C is a regularization parameter, larger C results in narrow margin and a smaller in a wider margin. N is just the number of lines we have in ours features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(W, X, Y):\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    distances[distances < 0] = 0    # the max between 0 and the distance is kept\n",
    "    hinge_loss = C * (np.sum(distances) / X.shape[0])\n",
    "    return 1 / 2 * np.dot(W, W) + hinge_loss ## = cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code of the gradient of the cost function:\n",
    "\n",
    "We can simplify $$max(0, 1-y_i * (w \\cdotp x_i + b))$$ into $$max(0, 1-y_i * (W \\cdotp X_i))$$ with $$W =(w,b)$$ and $$X = (x_i,1)$$\n",
    "and we obtain with the previous simplification:\n",
    "\n",
    "$$J(w) = \\frac{1}{2}||w||^2 + C\\left[\\frac{1}{N}\\sum\\limits_{i}^{n}max(0, 1-y_i * (W \\cdotp X_i))\\right]$$\n",
    "and finally, we have the following gradient of the cost function:\n",
    "\n",
    "$$\\nabla_w J(w) = \\frac{1}{N}\\sum\\limits_{i}^{n}\n",
    "\\begin{cases}\n",
    "  w & \\text{if } max(0, 1-y_i * (W \\cdotp X_i))=0 \\\\   \n",
    "  w-Cy_ix_i    & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function_gradient(W, X, Y):\n",
    "    Y = np.array([Y])\n",
    "    X = np.array([X]) \n",
    "    distances = 1 - (Y * np.dot(X, W))\n",
    "    grad = np.zeros(len(W))\n",
    "    for index, value in enumerate(distances):\n",
    "        if max(0, value) == 0:\n",
    "            dist = W\n",
    "        else:\n",
    "            dist = W - (C * Y[index] * X[index])\n",
    "        grad += dist\n",
    "    return grad/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to compute the stochastic gradient descent function:\n",
    "For this purpose, we have to minimise the two parts of the following equation:\n",
    "$$J(w) = \\frac{1}{2}||w||^2 + C\\left[\\frac{1}{N}\\sum\\limits_{i}^{n}max(0, 1-y_i * (W \\cdotp X_i))\\right]$$\n",
    "The gradient is the direction of the inscrease of the function J(w). We need to go to the direction of the decrease, that's why we calculate the gradient of the cost function from the train set. Particularly, we perform the gradient descent by substracting a learning rate multiplied by the gradient of the cost function from the weight initialized with zero value. And we compute the cost for all the 2^n. In this way, we can determine the weight by repeating the procedure a number of times we decide (here 2048 cycles). We can add a criterion to stop before the max_cycles value by comparing the difference between the previous cost and the new cost and if this mesurement is smaller than a percentage of the old cost, we stop the cycles and return directly the weigts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastic_gradient_descent(features, outputs):\n",
    "    max_cycles, rate = 2049, 0.001 \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    parameter, cycle = 0, 0\n",
    "    previous_cost = float(\"inf\")\n",
    "    while cycle <max_cycles:\n",
    "        cycle += 1\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        for index, value_of_X in enumerate(X):\n",
    "            ascention = compute_cost_function_gradient(weights, value_of_X , Y[index])\n",
    "            weights = weights - (learning_rate * ascention)\n",
    "        if cycle == 2**parameter:\n",
    "            cost = compute_cost_function(weights, features, outputs)\n",
    "            print(\"nb_of_cycles: {} and Cost: {}\".format(cycle, cost))\n",
    "            parameter +=1\n",
    "            print('cost=', cost)   \n",
    "            if abs(previous_cost - cost) < rate * previous_cost:\n",
    "                cycle = max_cycles\n",
    "            previous_cost = cost\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id        v1        v2        v3        v4        v5        v6  \\\n",
      "0     A0A2X4V775 -0.009765  0.200064 -0.104441 -0.939137 -0.015482 -0.243657   \n",
      "1     A0A5M9R2F2 -0.044548  0.192516 -0.147199 -0.894085 -0.009355 -0.330328   \n",
      "2     A0A4D6Y563 -0.063157  0.066017 -0.203133 -0.920998 -0.067741 -0.103863   \n",
      "3         P76341  0.036377  0.241014 -0.051348 -0.937244  0.013017 -0.207041   \n",
      "4         A0Z7F9  0.140453  0.229471 -0.070225 -0.960873 -0.028735 -0.324649   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "3995  A0A484GB52 -0.020858  0.087987 -0.106980 -0.970410 -0.041472 -0.208587   \n",
      "3996  A0A5Q4GKP0  0.045559  0.169432 -0.123162 -0.986888 -0.052270 -0.131644   \n",
      "3997  A0A2E4GIK9 -0.056719  0.077032 -0.154847 -0.982715 -0.055467 -0.096901   \n",
      "3998  A0A2M7CQW5  0.015895  0.210303 -0.065212 -0.978872 -0.021216 -0.095097   \n",
      "3999  A0A368CC27  0.020618  0.109883 -0.113834 -0.971576 -0.032970 -0.167549   \n",
      "\n",
      "            v7        v8        v9  ...      v184      v185      v186  \\\n",
      "0     0.118412  0.211521  0.356498  ... -0.055468 -0.116066 -0.031361   \n",
      "1     0.136786  0.223877  0.445563  ...  0.110012 -0.101992  0.008797   \n",
      "2     0.125786  0.234769  0.505547  ...  0.233741 -0.154388  0.137278   \n",
      "3     0.157360  0.192560  0.292982  ...  0.066501 -0.158044  0.086034   \n",
      "4     0.105875  0.147903  0.328881  ...  0.105572 -0.040058 -0.047729   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "3995  0.081806  0.245611  0.339105  ...  0.224782 -0.053446  0.067133   \n",
      "3996  0.070914  0.107376  0.414498  ... -0.013189 -0.002611  0.082772   \n",
      "3997  0.070613  0.146949  0.364325  ...  0.242932 -0.054235 -0.159938   \n",
      "3998  0.143493  0.260738  0.245225  ...  0.518518  0.011767  0.108016   \n",
      "3999  0.075435  0.204833  0.385751  ...  0.203668 -0.007218 -0.079477   \n",
      "\n",
      "          v187       v188      v189      v190      v191      v192  class  \n",
      "0    -0.050158  18.118879 -0.521866 -0.144021  2.924189 -0.278483      1  \n",
      "1    -0.008952  25.939377 -0.765972 -0.096016  3.780535 -0.587374      1  \n",
      "2    -0.054343  10.899848 -0.266273 -0.016061  3.576115  0.046467      1  \n",
      "3    -0.054905  17.253355 -0.334300 -0.147648  3.566811 -0.200707      1  \n",
      "4     0.008716  13.837421 -0.522097 -0.114400  1.796014  0.081138      1  \n",
      "...        ...        ...       ...       ...       ...       ...    ...  \n",
      "3995 -0.116225   7.760444 -0.185658 -0.187799  3.857352  0.009828      2  \n",
      "3996 -0.127592   8.392551 -0.210128 -0.016284  1.087825  0.066868      2  \n",
      "3997  0.185944   4.381233 -0.285145 -0.059999  1.744868  0.299367      2  \n",
      "3998 -0.108312   9.221044 -0.195858 -0.020499  2.877915 -0.033197      2  \n",
      "3999 -0.080238   8.907391 -0.142390 -0.162285  3.299746 -0.044643      2  \n",
      "\n",
      "[4000 rows x 194 columns]\n"
     ]
    }
   ],
   "source": [
    "C = 10000\n",
    "learning_rate = 0.000001\n",
    "#read and display dataset\n",
    "data = pd.read_csv('merge_cyto_periplasm.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id        v1        v2        v3        v4        v5        v6  \\\n",
      "0     A0A2X4V775 -0.009765  0.200064 -0.104441 -0.939137 -0.015482 -0.243657   \n",
      "1     A0A5M9R2F2 -0.044548  0.192516 -0.147199 -0.894085 -0.009355 -0.330328   \n",
      "2     A0A4D6Y563 -0.063157  0.066017 -0.203133 -0.920998 -0.067741 -0.103863   \n",
      "3         P76341  0.036377  0.241014 -0.051348 -0.937244  0.013017 -0.207041   \n",
      "4         A0Z7F9  0.140453  0.229471 -0.070225 -0.960873 -0.028735 -0.324649   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "3995  A0A484GB52 -0.020858  0.087987 -0.106980 -0.970410 -0.041472 -0.208587   \n",
      "3996  A0A5Q4GKP0  0.045559  0.169432 -0.123162 -0.986888 -0.052270 -0.131644   \n",
      "3997  A0A2E4GIK9 -0.056719  0.077032 -0.154847 -0.982715 -0.055467 -0.096901   \n",
      "3998  A0A2M7CQW5  0.015895  0.210303 -0.065212 -0.978872 -0.021216 -0.095097   \n",
      "3999  A0A368CC27  0.020618  0.109883 -0.113834 -0.971576 -0.032970 -0.167549   \n",
      "\n",
      "            v7        v8        v9  ...      v184      v185      v186  \\\n",
      "0     0.118412  0.211521  0.356498  ... -0.055468 -0.116066 -0.031361   \n",
      "1     0.136786  0.223877  0.445563  ...  0.110012 -0.101992  0.008797   \n",
      "2     0.125786  0.234769  0.505547  ...  0.233741 -0.154388  0.137278   \n",
      "3     0.157360  0.192560  0.292982  ...  0.066501 -0.158044  0.086034   \n",
      "4     0.105875  0.147903  0.328881  ...  0.105572 -0.040058 -0.047729   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "3995  0.081806  0.245611  0.339105  ...  0.224782 -0.053446  0.067133   \n",
      "3996  0.070914  0.107376  0.414498  ... -0.013189 -0.002611  0.082772   \n",
      "3997  0.070613  0.146949  0.364325  ...  0.242932 -0.054235 -0.159938   \n",
      "3998  0.143493  0.260738  0.245225  ...  0.518518  0.011767  0.108016   \n",
      "3999  0.075435  0.204833  0.385751  ...  0.203668 -0.007218 -0.079477   \n",
      "\n",
      "          v187       v188      v189      v190      v191      v192  class  \n",
      "0    -0.050158  18.118879 -0.521866 -0.144021  2.924189 -0.278483    1.0  \n",
      "1    -0.008952  25.939377 -0.765972 -0.096016  3.780535 -0.587374    1.0  \n",
      "2    -0.054343  10.899848 -0.266273 -0.016061  3.576115  0.046467    1.0  \n",
      "3    -0.054905  17.253355 -0.334300 -0.147648  3.566811 -0.200707    1.0  \n",
      "4     0.008716  13.837421 -0.522097 -0.114400  1.796014  0.081138    1.0  \n",
      "...        ...        ...       ...       ...       ...       ...    ...  \n",
      "3995 -0.116225   7.760444 -0.185658 -0.187799  3.857352  0.009828   -1.0  \n",
      "3996 -0.127592   8.392551 -0.210128 -0.016284  1.087825  0.066868   -1.0  \n",
      "3997  0.185944   4.381233 -0.285145 -0.059999  1.744868  0.299367   -1.0  \n",
      "3998 -0.108312   9.221044 -0.195858 -0.020499  2.877915 -0.033197   -1.0  \n",
      "3999 -0.080238   8.907391 -0.142390 -0.162285  3.299746 -0.044643   -1.0  \n",
      "\n",
      "[4000 rows x 194 columns]\n"
     ]
    }
   ],
   "source": [
    "#convert existing labels into 1 et -1 labels\n",
    "diag_map = {1: 1.0, 2: -1.0}\n",
    "data['class'] = data['class'].map(diag_map)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign labels and features to different data frames\n",
    "Y = data.loc[:, 'class']\n",
    "X = data.iloc[:, 1:193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize values of features to avoid overflow\n",
    "X_normalized = MinMaxScaler().fit_transform(X.values)\n",
    "X = pd.DataFrame(X_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a new column b full of 1 at the end\n",
    "X.insert(loc=len(X.columns), column='b', value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset to obtain train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start\n",
      "nb_of_cycles: 1 and Cost: 799.9733285398822\n",
      "cost= 799.9733285398822\n",
      "nb_of_cycles: 2 and Cost: 558.163733566956\n",
      "cost= 558.163733566956\n",
      "nb_of_cycles: 4 and Cost: 475.9975838082105\n",
      "cost= 475.9975838082105\n",
      "nb_of_cycles: 8 and Cost: 455.8999913014187\n",
      "cost= 455.8999913014187\n",
      "nb_of_cycles: 16 and Cost: 378.2663782243924\n",
      "cost= 378.2663782243924\n",
      "nb_of_cycles: 32 and Cost: 327.32891585969065\n",
      "cost= 327.32891585969065\n",
      "nb_of_cycles: 64 and Cost: 301.23904110024125\n",
      "cost= 301.23904110024125\n",
      "nb_of_cycles: 128 and Cost: 322.8423226987466\n",
      "cost= 322.8423226987466\n",
      "nb_of_cycles: 256 and Cost: 259.04151276435647\n",
      "cost= 259.04151276435647\n",
      "nb_of_cycles: 512 and Cost: 280.1468152150309\n",
      "cost= 280.1468152150309\n",
      "nb_of_cycles: 1024 and Cost: 329.6807707259754\n",
      "cost= 329.6807707259754\n",
      "nb_of_cycles: 2048 and Cost: 263.7403646072589\n",
      "cost= 263.7403646072589\n",
      "training end\n",
      "weights are: [ 7.50694442e-01  2.77537955e+00 -1.18015851e+00 -9.74146563e-01\n",
      "  1.60852501e+00 -1.39915874e-01  9.62205857e-01 -2.47476568e+00\n",
      "  3.21783951e-01 -3.02188149e-01 -2.71115463e-01 -8.98410141e-01\n",
      "  9.70145858e-01 -3.04611541e+00  8.23540793e-01  1.54396824e+00\n",
      "  2.48016928e-01  7.30027361e-01  4.03984760e-01 -3.63135103e-01\n",
      " -9.45997253e-01 -2.26873178e-01 -1.72522342e+00  1.62326827e+00\n",
      " -1.17136081e+00  2.55698663e+00 -1.06579216e+00 -2.33454869e+00\n",
      "  8.07540640e-01  8.11338771e-01 -2.34507277e+00  4.07991367e-01\n",
      "  2.24286779e+00 -2.66432597e+00 -1.35695337e+00  2.29183410e+00\n",
      " -1.29872236e+00  2.03979963e+00 -4.02802322e-02 -1.98129564e+00\n",
      " -1.22035314e-01  1.87034124e+00  4.37541647e-01  2.00401715e+00\n",
      "  1.65165873e+00  3.88757220e-01 -1.15706967e+00  1.18756464e-01\n",
      " -2.92000285e-01  6.63614592e-02  5.55525427e-01  2.44831608e+00\n",
      "  3.26832222e-01  2.39361803e-01 -8.36521958e-01  2.69627067e+00\n",
      "  1.72313156e-01  7.03258547e-01  7.39953405e-01  5.10429552e-01\n",
      " -1.36423318e+00 -2.30455260e+00  2.62761854e+00  1.31491311e+00\n",
      " -3.02621684e-02  1.37594408e-01 -9.91885314e-01  5.84468548e-01\n",
      " -3.20316792e-01  5.49647594e-01  2.15926930e-01  3.25513819e-01\n",
      " -6.02547659e-01 -5.74551177e-01  6.17626141e-01 -1.79883519e-04\n",
      " -3.39595407e-01 -2.99166780e-01  1.86255861e-01  1.92505376e-03\n",
      "  7.00597215e-02  1.00936847e-01  2.03293788e-01  4.01655790e-01\n",
      " -1.11194561e+00  2.22279974e-01  1.47360763e-01  3.93728212e-01\n",
      " -2.91383710e-01  4.46534666e-01 -3.19866958e-01  3.52767324e-01\n",
      "  1.06259662e-01  4.78681029e-01 -3.08682895e-01 -6.04208689e-01\n",
      "  1.31226311e+00 -3.60075417e-02  6.80432473e-01  2.86058243e-01\n",
      "  5.62376538e-01 -8.65419651e-01  3.46913973e-01  1.49590122e-02\n",
      "  4.69611660e-01 -6.21034935e-01  1.85791540e-01 -5.03133863e-01\n",
      " -2.32729792e-01  1.69481992e-02 -1.31993056e-01 -6.90567532e-01\n",
      "  7.49160653e-01  2.47121237e-01  7.21650770e-01  7.46543726e-01\n",
      "  1.18973040e-01 -1.23018478e+00  2.49700387e-01  8.84231612e-02\n",
      "  5.52575883e-01 -3.15873678e-01 -4.70930638e-01 -5.33550969e-01\n",
      " -1.57936705e+00 -1.92398989e-01 -1.84180881e-02  7.72236956e-03\n",
      " -2.04110099e-01 -6.09459947e-01 -1.42034500e-01 -2.72450517e-01\n",
      " -5.23058897e-01  3.93635358e-01  2.54140891e-01 -9.36188406e-01\n",
      " -2.13209167e-01 -5.97821313e-01  4.38278435e-01 -6.66512572e-01\n",
      "  1.72125063e-01 -4.34695776e-01  3.70770803e-01  1.25945855e+00\n",
      " -9.52738344e-01  7.15948151e-02  1.01038209e+00 -6.34328643e-01\n",
      " -1.10407801e+00  1.72151330e+00 -9.12813794e-01  2.46951237e-01\n",
      " -1.10606535e+00 -1.00727210e-01  7.85038051e-02  2.18367487e-01\n",
      " -6.76715769e-01 -5.28443770e-01 -8.22577424e-01 -7.06842610e-01\n",
      " -7.33907117e-01  6.45164953e-01 -8.96998947e-01 -2.55905440e-01\n",
      " -2.13686025e-01 -9.48932601e-01 -1.33901872e-01  2.65476760e-01\n",
      "  4.16089760e-01  6.04417877e-01  4.26228626e-01  2.77270955e-01\n",
      "  2.82575050e-01 -1.67352609e-01 -8.93963490e-01 -9.69467248e-02\n",
      "  3.10170236e-02 -5.49433268e-01  3.81229010e-01  1.02589867e+00\n",
      " -3.18252129e-01  1.85394483e-02  9.12136602e-01 -6.69105609e-01\n",
      "  5.19519064e-01 -9.81816488e-02  2.06121769e-02  4.66155067e-01\n",
      " -4.61150933e-01 -4.35266836e-01 -1.19337547e-02  2.39660012e-01\n",
      "  4.03295377e-01]\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "print(\"training start\")\n",
    "W = compute_stochastic_gradient_descent(X_train.to_numpy(), y_train.to_numpy())\n",
    "print(\"training end\")\n",
    "print(\"weights are: {}\".format(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply the model on test set:\n",
      "accuracy of svm on test dataset: 0.9825\n",
      "precision of svm on test dataset: 0.9811320754716981\n"
     ]
    }
   ],
   "source": [
    "print(\"apply the model on test set:\")\n",
    "y_train_predicted = np.array([])\n",
    "for i in range(X_train.shape[0]):\n",
    "    ypred = np.sign(np.dot(X_train.to_numpy()[i], W))\n",
    "    y_train_predicted = np.append(y_train_predicted, ypred)\n",
    "\n",
    "y_test_predicted = np.array([])\n",
    "for i in range(X_test.shape[0]):\n",
    "    ypred = np.sign(np.dot(X_test.to_numpy()[i], W))\n",
    "    y_test_predicted = np.append(y_test_predicted, ypred)\n",
    "\n",
    "print(\"accuracy of svm on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "print(\"precision of svm on test dataset: {}\".format(precision_score(y_test, y_test_predicted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
