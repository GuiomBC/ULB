{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Computational Game TheoryAssignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 1. N-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "My roll number ends with **3**.\n",
    "Therefore, I have taken **table 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![The cumulative average of the different selection methods over time. \\label{cumave}](ex1.1/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 1.\\label{arm1}](ex1.1/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 2.\\label{arm2}](ex1.1/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 3.\\label{arm3}](ex1.1/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 4.\\label{arm4}](ex1.1/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Random selection method. \\label{his_random}](ex1.1/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0).\\label{his_greedy0}](ex1.1/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0.1).\\label{his_greedy1}](ex1.1/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0.2).\\label{his_greedy2}](ex1.1/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Softmax selection method (tau=1).\\label{his_softmax1}](ex1.1/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Softmax selection method (tau=0.1).\\label{his_softmax01}](ex1.1/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Random selection\n",
    "\n",
    "Each round, the random selection method will select a random arm, no matter what the $Q_{ai}$ values.\n",
    "This means that each arm is selected about the same amount of times as is shown in the histogram, constructed out of 100 replicates (Fig. \\ref{his_random}).\n",
    "The cumulative average reward will be the average of the rewards of the different arms.\n",
    "This value is 1.725 (Fig. \\ref{cumave}).\n",
    "Because each arm is selected around 250 times, there is ample opportunity for the $Q_{ai}$ values to converge to $Q_{ai}^*$ (Fig. \\ref{arm1}-\\ref{arm4})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Greedy selection (epsilon=0)\n",
    "\n",
    "Greedy selection will select the action with highest $Q_{ai}$ value. \n",
    "Because this is also the only action for which the $Q_{ai}$ value is updated, \n",
    "this is the only selected method as long as $Q_{ai}$ stays above 0.\n",
    "In this case all actions start with $Q_{ai}=0$.\n",
    "By lucky chance arm 1 is the optimal action, so this was always chosen (Fig. \\ref{his_greedy0}), giving the optimal cumulative reward of 2.4 (Fig. \\ref{cumave}).\n",
    "If however, arm 4 would have been the optimal action, the greedy selection would wrongly assume arm 1 to be the optimal one, since they all start with an initial value 0, but arm 1 is the first action encountered and thus taken. \n",
    "A better way would be to try every arm at least once and than chose an optimal action.\n",
    "But even then, this could be wrong, because the rewards are drawn from a normal distribution.\n",
    "It is possible that by chance, arm 4 yields an higher rewards than arm 1, even though on average it does not.\n",
    "\n",
    "For the greedy selection, only the $Q_{ai}$ value of arm 1 converges towards $Q_{ai}^*$, since this is the only one that get updated (Fig. \\ref{arm1}-\\ref{arm4})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Greedy selection (epsilon=0.1 and 0.2)\n",
    "\n",
    "A Greedy selection with epsilon 0.10 will chose a random arm 10 percent of the time and chose greedy otherwise.\n",
    "A greedy selection with epsilon 0.20 will chose a random arm 20 percent of the time.\n",
    "The more a selection method chooses randomly, the faster it converges toward the correct $Q_{ai}^*$ values (Fig. \\ref{arm1}-\\ref{arm4}).\n",
    "However this comes at a cost, if you chose 20 percent of the time a random value, you are only get the optimal reward for 80 percent of the time. The 20 remaining percent of the time you will get a random reward.\n",
    "In general, a higher epsilon results in faster convergence towards the $Q_{ai}^*$ values, but a lower final cumulative average reward (Fig \\ref{cumave}).\n",
    "\n",
    "In Fig. \\ref{his_greedy1} and \\ref{his_greedy2}, we see that in most replicates arm 1 is selected the most times.\n",
    "However in some replicates, arm 4 is selected more often than arm 1.\n",
    "This is when by chance arm 4 gives a higher reward than arm 1 in the beginning of the process. \n",
    "The greedy selection method than assumes that arm 4 is the optimal action.\n",
    "Because it only selects randomly 10 or 20 percent of the time, the $Q_{ai}$ value of arm 4 will grow faster towards its $Q_{ai}^*$ value than the $Q_{ai}$ of arm 1.\n",
    "Therefore it is possible that for X rounds (up to 1000), arm 4 is assumed to be the optimal action, even though that is not the case.\n",
    "With infinite rounds however, this wrong would be corrected and arm 1 will be considered the optimal action by the selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Softmax selection (tau=1 and 0.1)\n",
    "\n",
    "In the softmax selection method, an higher tau represent an higher temperature.\n",
    "The higher the temperature, the more randomness in the system.\n",
    "In the limits, a tau of infinite would result in all actions selected by equal probability.\n",
    "A tau of 0 zero (or almost because you can not divide by zero, would result in the action with the highest $Q_{ai}$ value to be chosen, even if it is only marginally better than the second best action.\n",
    "\n",
    "Therefore, when the tau value is to low, the selection method will not always find the optimal action.\n",
    "This can be seen in Fig \\ref{cumave} where the Softmax selection method with tau 0.1 results in a cumulated average of rewards much lower than the optimal value. \n",
    "Remember that this graph only show the average over 100 replicates,\n",
    "in individual cases, it can converge towards the optimal reward of 2.4.\n",
    "When the tau value is to high however, there is to much randomness in the system and and the optimal actions is not chosen often enough.\n",
    "\n",
    "As with the other selection methods, arms that get chosen more often, have a higher chance to converge toward $Q_{ai}^*$. \n",
    "So relatively, arms with a high reward (arm 1 and arm 4) will converge faster with a tau of 0.1.\n",
    "arms with a lower reward (arms 2 and 3) will converge faster with a tau of 1.\n",
    "However this is not what we see for tau 0.1 in (Fig. \\ref{arm1} and \\ref{arm4}).\n",
    "The $Q_{ai}$ converges towards a value much lower than $Q_{ai}^*$.\n",
    "This is again because the graphs show the average of 100 replicates.\n",
    "\n",
    "Fig. \\ref{his_softmax01} shows that in individual cases, \n",
    "the softmax selection with tau 0.1 is very sensitive toward choosing a non optimal action in the beginning.\n",
    "Because this will result in an increase of  $Q_{ai}$ value for that specific action and other actions are not chosen enough to catch up, \n",
    "that action will be wrongly assumed to be the optimal one.\n",
    "\n",
    "A way to mitigate this problem for low tau values a little bit is by with optimistic softmax selection. \n",
    "here we start with initial $Q_{ai}$ values much higher than the real ones.\n",
    "Even with a low tau value, the optimal action has a high chance of converging towards $Q_{ai}^*$.\n",
    "The other actions will be overestimated in $Q_{ai}$ values a somewhere below the $Q_{ai}^*$ value of the optimal action.\n",
    "\n",
    "### Conclusion\n",
    "Greedy selection is best method if there is low variance on the reward.\n",
    "If there is variance, it is best to use greedy selection with an epsilon as low as you have patience.\n",
    "Selection methods with lower epsilon values will take longer to converge toward the optimal action, but will have an higher cumulated average reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\newpage\n",
    "\\newpage\n",
    "\n",
    "## 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![The cumulative average of the different selection methods over time. \\label{2cumave}](ex1.2/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 1.\\label{2arm1}](ex1.2/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 2.\\label{2arm2}](ex1.2/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 3.\\label{2arm3}](ex1.2/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 4.\\label{2arm4}](ex1.2/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Random selection method. \\label{2his_random}](ex1.2/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0).\\label{2his_greedy0}](ex1.2/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0.1).\\label{2his_greedy1}](ex1.2/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0.2).\\label{2his_greedy2}](ex1.2/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Softmax selection method (tau=1).\\label{2his_softmax1}](ex1.2/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Softmax selection method (tau=0.1).\\label{2his_softmax01}](ex1.2/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Increasing the Variance of each reward somehow had (in this case) little effect on the relative performance of each selection method.\n",
    "Greedy selection is still the best method with the same rules concerning the epsilon value.\n",
    "\n",
    "The $Q_{ai}$ evolution graphs show more fluctuations, but are except for that very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\newpage\n",
    "\\newpage\n",
    "\n",
    "## 1.3\n",
    "### greedy with decreasing epsilon\n",
    "\n",
    "Greedy selection with decreasing epsilon needs more time for the $Q_{ai}$ values to converge toward $Q_{ai}^*$.\n",
    "However, during the initial rounds it samples more than the greedy methods with a fixed value.\n",
    "\n",
    "within 1000 rounds the cumulative average reward did not converge toward a fixed value. Near the end, it had surpassed greedy method with epsilon 0.2.\n",
    "However, given time, I suspect the greedy method with decreasing epsilon to converge toward the optimal cumulative average reward value of 2.4.\n",
    "This is not possible with a fixed epsilon value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![The cumulative average of the different selection methods over time. \\label{3cumave}](ex1.3a/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 1.\\label{3arm1}](ex1.3a/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 2.\\label{3arm2}](ex1.3a/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 3.\\label{3arm3}](ex1.3a/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 4.\\label{3arm4}](ex1.3a/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy decreasing epsilon \\label{3his_random}](ex1.3a/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0).\\label{3his_greedy0}](ex1.3a/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0.1).\\label{3his_greedy1}](ex1.3a/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Greedy selection method (epsilon=0.2).\\label{3his_greedy2}](ex1.3a/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\newpage\n",
    "\\newpage\n",
    "\n",
    "### Softmax with decreasing tau\n",
    "\n",
    "Softmax selection with decreasing tau has more chance to converge toward the right $Q_{ai}^*$ values and it does this faster.\n",
    "\n",
    "within 1000 rounds the cumulative average reward did not converge toward a fixed value. \n",
    "However, given time, I suspect the softmax method with decreasing tau to converge toward the optimal cumulative average reward value of 2.4.\n",
    "This is not possible with a fixed epsilon value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![The cumulative average of the different selection methods over time. \\label{4cumave}](ex1.3b/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 1.\\label{4arm1}](ex1.3b/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 2.\\label{4arm2}](ex1.3b/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 3.\\label{4arm3}](ex1.3b/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Evolution of Qai values for arm 4.\\label{4arm4}](ex1.3b/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Softmax decreasing tau \\label{4his_random}](ex1.3b/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Softmax selection method (tau=1).\\label{4his_greedy0}](ex1.3b/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Histogram that shows how many times each arm was selected over 1000 rounds. Constructed with 100 replicates. Softmax selection method (tau=0.1).\\label{4his_greedy1}](ex1.3b/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\newpage\n",
    "\\newpage\n",
    "\n",
    "# 2. Stochastic Reward Game\n",
    "## 2.1 Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![](ex2/1.png)\n",
    "\n",
    "![](ex2/2.png)\n",
    "\n",
    "![](ex2/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In General, the optimistic Boltzmann selection method has more chance of coming to the optimal joint action as compared to the regular Boltzmann selection method.\n",
    "However, even the optimistic Boltzmann value does not get to the optimal joint action all the time.\n",
    "We can conclude this from the graph since the the average collected reward does not converge toward 11, \n",
    "which would be the maximum value.\n",
    "This means that of the 50 replicates, some reached the optimal joint action, and other did not.\n",
    "\n",
    "Each agent has to form believes about which action the other agent will do.\n",
    "I implemented this by remembering the last N actions of the agents and calculating the probability for each action to occur.\n",
    "I found that making N smaller makes it more easy for the agents to reach the optimal joint action with reward 11.\n",
    "If all previous actions would be taken into account, \n",
    "agents would have great difficulty to \"change their mind\", because by the time they find out that joint action (a1, b1) gives a high reward, they will have acted (a2, b2) many times. This way, the agents will give a small probability toward the other agent choosing action a1 or b1.\n",
    "So, when the calculate the weighted reward of for example choosing action a1,\n",
    "this will be dominated by joint action (a1, b2), which gives a negative reward.\n",
    "Therefore it is not likely that the agents would change there action.\n",
    "\n",
    "By using the optimistic Boltzmann selection, joint action (a1, b1) is explored much more during the early episodes.\n",
    "Therefore, they have time to discover the high reward of joint action (a1, b1) before they discover the negative rewards of joint actions (a1, b2) and (a2, b1) (even more because these joint actions also start with a very optimistic estimation of the reward).\n",
    "So, optimistic Boltzmann selection converges toward the optimal joint action more often than the regular Boltzmann selection.\n",
    "\n",
    "### effect of higher sigma\n",
    "\n",
    "Changing the sigma, sigma0 or sigma1 seems to have little effect on the optimistic Boltzmann selection.\n",
    "Increasing the sigma1 slightly decreases the performance of the regular Boltzmann selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\newpage\n",
    "\\newpage\n",
    "\n",
    "## 2.2 Discussion\n",
    "### 1.  How will the learning process change if we make the agents independent learners?\n",
    "\n",
    "If the agents are independent, they do not know what the actions of each other.\n",
    "They only know their own action and the reward they receive.\n",
    "In practice, they will converge toward the same result as joint action learners, but a bit slower.\n",
    "It does not affect the agents so much as you would expect.\n",
    "\n",
    "### 2.  How will the learning process change if we make the agents always select the action that according to them will yield the highest reward (assuming the other agent plays the best response)?\n",
    "\n",
    "I suspect this would speed up the learning process a lot.\n",
    "As soon as one agent discovers that joint action (a1, b1) yields the highest reward, it will keep selecting action a1 or a2 every time they are not exploring.\n",
    "The other agent will refuse to take action a1 or b1 as long as he thinks this will not result in the highest reward.\n",
    "However, every time he explores randomly and happens to choose a1 or b1, this will result in joint action (a1, b1).\n",
    "The believe of the reward will be updated until both agents know joint actions (a1, b1) yields the highest reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
